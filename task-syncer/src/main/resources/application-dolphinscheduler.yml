spring:
  datasource:
    dynamic:
      primary: diagnose
      strict: false
      datasource:
        diagnose:
          url: jdbc:mysql://localhost:33066/compass?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai&tinyInt1isBit=false
          username: root
          password: root
        source:
          url: jdbc:mysql://localhost:33066/dolphinscheduler?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai&tinyInt1isBit=false
          username: root
          password: root

  kafka:
    bootstrap-servers: localhost:9095
    topics: mysqldata
    consumer:
      group-id: task-syncer-ds
      auto-offset-reset: earliest
      enable-auto-commit: false
      max-poll-interval-ms: 300000


# Synchronize mysql binlog data from kafka
# Required mapping table: user, project, flow, task, task_instance
datasource:
  mappings:
    # user table mapping
    - schema: "dolphinscheduler" # Sync source database
      table: "t_ds_user"         # Source table name
      targetTable: "user"        # Target table name
      columnMapping: # Field mapping, mapping id, user_name... fields to user_id, username...
        user_id: "id"            # Target field: source field
        username: "user_name"
        password: "user_password"
        is_admin: "user_type"
        email: "email"
        phone: "phone"
        create_time: "create_time"
        update_time: "update_time"
      columnValueMapping: # Field value mapping, target field value, source field value, field type
        is_admin: [ { targetValue: "0", originValue: [ "0" ] }, { targetValue: "1", originValue: [ "1" ] } ]
      constantColumn:
        scheduler_type: "DolphinScheduler"

    # project table mapping
    - schema: "dolphinscheduler"
      table: "t_ds_project"
      targetTable: "project"
      columnMapping:
        id: "id"
        project_name: "name"
        description: "description"
        user_id: "user_id"
        project_status: "flag"
        create_time: "create_time"
        update_time: "update_time"
      columnValueMapping: # Field value mapping
        project_status: [ { targetValue: "0", originValue: [ "0" ] }, { targetValue: "1", originValue: [ "1" ] } ]

    # flow table mapping
    - schema: "dolphinscheduler"
      table: "t_ds_process_definition"
      targetTable: "flow"
      columnMapping:
        id: "id"
        flow_name: "name"
        description: "description"
        user_id: "user_id"
        flow_status: "flag"
        project_id: ""
        project_name: ""
        create_time: "create_time"
        update_time: "update_time"
      columnDep:
        columns: [ "project_id", "project_name" ] # Depends on project_id, project_name, needs to be fetched from other tables
        queries: [ "select project.id as project_id, project.name as project_name from t_ds_process_definition as process inner join t_ds_project as project on process.project_code=project.code where process.id=${id}" ]

    # task table mapping
    - schema: "dolphinscheduler"
      table: "t_ds_task_definition"
      targetTable: "task"
      columnMapping:
        id: "id"
        project_name: ""
        project_id: ""
        flow_name: ""
        flow_id: ""
        task_name: "name"
        description: "description"
        user_id: "user_id"
        task_type: "task_type"
        retries: "fail_retry_times"
        create_time: "create_time"
        update_time: "update_time"
      columnDep:
        columns: [ "project_id","project_name","flow_name","flow_id" ] # t_ds_task_definition, t_ds_process_task_relation, t_ds_process_definition, t_ds_project
        queries: [ "select process.id as flow_id, process.name as flow_name, project.id as project_id, project.name as project_name from t_ds_task_definition as task inner join t_ds_process_task_relation as relation on task.code=relation.post_task_code inner join t_ds_process_definition as process on relation.process_definition_code=process.code inner join t_ds_project as project on task.project_code=project.code where task.id=${id}" ]

    # task_instance table mapping
    - schema: "dolphinscheduler"
      table: "t_ds_task_instance"
      targetTable: "task_instance"
      columnMapping:
        id: "id"
        project_name: ""
        flow_name: ""
        task_name: "name"
        start_time: "start_time"
        end_time: "end_time"
        execution_time: ""
        task_state: "state"  # mapping
        task_type: "task_type"
        retry_times: "retry_times"
        max_retry_times: "max_retry_times"
        worker_group: "worker_group"
        create_time: "create_time"
        update_time: "update_time"
      columnValueMapping: # Field value mapping
        # 0 commit succeeded, 1 running, 2 prepare to pause, 3 pause, 4 prepare to stop, 5 stop, 6 fail, 7 succeed, 8 need fault tolerance, 9 kill, 10 wait for thread, 11 wait for dependency to complete'
        # Only need to pay attention to success and failure, other states are other
        task_state:
          - { targetValue: "success", originValue: [ "7", "14" ] }
          - { targetValue: "fail", originValue: [ "6", "9" ] } # kill is also fail
          - { targetValue: "other", originValue: [ "0","1", "2", "3", "4", "5", "8", "10", "11", "12", "13" ] }
      columnDep:
        columns: [ "project_name", "flow_name", "execution_time" ]
        queries: [ "select t2.schedule_time as execution_time, t3.name as flow_name, t4.name as project_name from t_ds_task_instance as t1 inner join t_ds_process_instance as t2 on t1.process_instance_id = t2.id inner join t_ds_process_definition as t3 on t2.process_definition_code = t3.code inner join t_ds_project as t4 on t3.project_code=t4.code where t1.id=${id}" ]
      # Write back to kafka topic
      writeKafkaTopic: task-instance

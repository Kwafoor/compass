spring:
  datasource:
    url: jdbc:mysql://localhost:33066/compass?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai
    username: root
    password: root
    druid:
      initial-size: 5
      min-idle: 10
      max-active: 20
  kafka:
    bootstrap-servers: "localhost:9095"
    topics: "task-instance"
    consumer:
      group-id: "cp-task-application"
      auto-offset-reset: "earliest"
      max-poll-interval-ms: 300000
    taskApplicationTopic: "task-application"
  redis:
    cluster:
      nodes: localhost:6379
      max-redirects: 3
    password:
    lettuce:
      pool:
        max-active: 32
        max-idle: 16
        min-idle: 8

custom:
  delayedTask:
    enable: true
    queue: "{lua}:task:application"
    processing: "{lua}:task-processing"
    delayedSeconds: 5
    tryTimes: 20
  # parse through to the task's application ID serially from top to bottom
  rules:
    - logPathDep: # log dependency query
        query: "SELECT CASE WHEN end_time IS NOT NULL THEN end_time ELSE start_time END as end_time,log_path FROM t_ds_task_instance WHERE id=${id}"     # task-instance id
      logPathJoins: # compose log path
        # end_time: 2023-02-18 01:43:11
        # log_path: ../logs/6354680786144_1/3/4.log
        - { "column": "", "data": "/flume/dolphinscheduler" } # configure the HDFS root directory for storing scheduled logs
        - { "column": "end_time", "regex": "^.*(?<date>\\d{4}-\\d{2}-\\d{2}).+$", "name": "date" }
        - { "column": "log_path", "regex": "^.*logs/(?<logpath>.*)$", "name": "logpath" }
      extractLog: # parse logs based on the assembled log path
        regex: ".*(?<applicationId>application_[0-9]+_[0-9]+).*$"     # matching rules
        name: "applicationId"      # match a text name that must end with the application ID

spring:
  datasource:
    url: jdbc:mysql://localhost:33066/compass?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai
    username: root
    password: root
    druid:
      initial-size: 5
      min-idle: 10
      max-active: 20
  kafka:
    bootstrap-servers: "localhost:9095"
    topics: "task-instance"
    consumer:
      group-id: "cp-task-application"
      auto-offset-reset: "earliest"
      max-poll-interval-ms: 300000
  redis:
    cluster:
      nodes: localhost:6379
      max-redirects: 3
    password:
    lettuce:
      pool:
        max-active: 32
        max-idle: 16
        min-idle: 8

custom:
  delayedTask:
    enable: true
    queue: "{lua}:task:application"
    processing: "{lua}:task-processing"
    delayedSeconds: 5
    tryTimes: 20
  # parse through to the task's Application ID serially from top to bottom
  rules:
    - logPathDep: # log dependency query
        query: ""    # variable dependency query
      logPathJoins: # concatenate log absolute path
        - { "column": "", "data": "/flume/airflow" } # configure the HDFS root directory for storing scheduled logs
        - { "column": "flow_name",data: "dag_id=", "regex": "(?<flowName>.*)","name": "flowName" }
        - { "column": "run_id",data: "run_id=", "regex": "(?<runId>.*)", "name": "runId" }
        - { "column": "task_name",data: "task_id=","regex": "(?<taskName>.*)", "name": "taskName" }
        - { "column": "retry_times",data: "attempt=", "regex": "(?<fileName>.*)", "name": "fileName" }
      extractLog: # parse logs based on the assembled log path
        regex: ".*(?<applicationId>application_[0-9]+_[0-9]+).*$"     # matching rules
        name: "applicationId"      # match text name, must end with application ID
